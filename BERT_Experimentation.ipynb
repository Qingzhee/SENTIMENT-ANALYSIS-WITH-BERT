{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DistillBERT Experimentation\n",
        "\n",
        "In this notebook, we explore sentiment analysis on movie reviews using DistilBERT. Although our initial plan was to use the full BERT model, limited resources led us to switch to DistilBERT.\n",
        "\n",
        "During preprocessing, we observed that some reviews exceeded the 512-token limit. To handle this, we experimented with two approaches: truncation and chunking.\n",
        "\n",
        "We then compared two training strategies: (1) fine-tuning all pre-trained DistilBERT layers, and (2) freezing DistilBERT and training two additional layers on top.\n",
        "\n",
        "Conclusion: Truncation outperformed chunking, and fine-tuning the full DistilBERT model yielded better results than training only the added layers."
      ],
      "metadata": {
        "id": "EOYXUD17I76-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DistilBertModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "ZeuFoTxrxbg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6ayULONTvs2",
        "outputId": "fd558c9d-a6f8-467a-9f5b-9863ebb42d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC2BMSM9oEEa",
        "outputId": "33a46b57-c6af-48e5-9b23-227329eab4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preperation"
      ],
      "metadata": {
        "id": "k-vww4ZYJD0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pos_df = pd.read_csv('/content/drive/MyDrive/it1244/cleanedposfull.csv')\n",
        "neg_df = pd.read_csv('/content/drive/MyDrive/it1244/cleanednegfull.csv')\n",
        "\n",
        "pos_df['label'] = 1  # Positive = 1\n",
        "neg_df['label'] = 0  # Negative = 0\n",
        "\n",
        "pos_df = pos_df[['FileName', 'Cleaned_Content', 'label']]\n",
        "neg_df = neg_df[['FileName', 'Cleaned_Content', 'label']]\n",
        "\n",
        "df = pd.concat([pos_df, neg_df], axis=0, ignore_index=True)\n",
        "\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Combined dataset shape:\", df.shape)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqgyVvp9x2vI",
        "outputId": "a901397b-b9da-485c-8427-928a551034cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Combined dataset shape: (50000, 3)\n",
            "    FileName                                    Cleaned_Content  label\n",
            "0  17568.txt  and how they bore you right out of your mind t...      0\n",
            "1  14894.txt  its not citizen kane but it does deliver cleav...      1\n",
            "2  23805.txt  if you like othello youll love this flick sinc...      1\n",
            "3  13159.txt  i watched the this the other night on a local ...      1\n",
            "4  10128.txt  well i am so glad i watched this on hbo instea...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets.\n",
        "# We use an 80-20 split, where 20% of the data is reserved for validation.\n",
        "# Stratification is applied on the 'label' column to ensure that both sets have a similar class distribution.\n",
        "# The random_state parameter is set to 42 to ensure reproducibility of the split.\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", train_df.shape)\n",
        "print(\"Validation set size:\", val_df.shape)\n",
        "\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(train_df['label'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nClass distribution in validation set:\")\n",
        "print(val_df['label'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHbejcJDx-d7",
        "outputId": "436807b0-37aa-4903-876c-72b946ff6b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (40000, 3)\n",
            "Validation set size: (10000, 3)\n",
            "\n",
            "Class distribution in training set:\n",
            "label\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class distribution in validation set:\n",
            "label\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained DistilBERT tokenizer (uncased version) for tokenizing input text.\n",
        "# This tokenizer converts raw text into tokens and corresponding IDs, which are used as inputs to the model.\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n"
      ],
      "metadata": {
        "id": "CWp5UT4Rh2hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Truncation"
      ],
      "metadata": {
        "id": "g3Y6Z19qkfcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training data:\n",
        "train_encodings = tokenizer(\n",
        "    train_df['Cleaned_Content'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Tokenize the validation data:\n",
        "val_encodings = tokenizer(\n",
        "    val_df['Cleaned_Content'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n"
      ],
      "metadata": {
        "id": "Cms3T2jMyJng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for movie reviews that inherits from PyTorch's Dataset\n",
        "class MovieReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Create training and validation datasets using the custom MovieReviewDataset class\n",
        "train_dataset = MovieReviewDataset(train_encodings, train_df['label'].tolist())\n",
        "val_dataset   = MovieReviewDataset(val_encodings, val_df['label'].tolist())"
      ],
      "metadata": {
        "id": "VwWmmu2yzVDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained DistilBERT model for sequence classification with two output labels\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "id": "Nt74DJdYh9va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments, which specify hyperparameters and configurations for training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/it1244/results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K2HuZQY0Aha",
        "outputId": "e8c6dac8-87a5-49f5-914d-ea242adfaf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Convert logits to predicted class\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xVqPWKC30zax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Trainer object that handles the training loop and evaluation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "gVdKYZZE02LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "9sk3Mlnw1KP8",
        "outputId": "f46727ce-5367-4ad5-8aba-46588c5c3786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqingzhe2\u001b[0m (\u001b[33mqingzhe2-national-university-of-singapore-students-union\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250402_040813-e7u8m7d4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/qingzhe2-national-university-of-singapore-students-union/huggingface/runs/e7u8m7d4' target=\"_blank\">/content/drive/MyDrive/it1244/results</a></strong> to <a href='https://wandb.ai/qingzhe2-national-university-of-singapore-students-union/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/qingzhe2-national-university-of-singapore-students-union/huggingface' target=\"_blank\">https://wandb.ai/qingzhe2-national-university-of-singapore-students-union/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/qingzhe2-national-university-of-singapore-students-union/huggingface/runs/e7u8m7d4' target=\"_blank\">https://wandb.ai/qingzhe2-national-university-of-singapore-students-union/huggingface/runs/e7u8m7d4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 1:08:45, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.363000</td>\n",
              "      <td>0.253300</td>\n",
              "      <td>0.923700</td>\n",
              "      <td>0.911120</td>\n",
              "      <td>0.939000</td>\n",
              "      <td>0.924850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.117000</td>\n",
              "      <td>0.278667</td>\n",
              "      <td>0.932600</td>\n",
              "      <td>0.924451</td>\n",
              "      <td>0.942200</td>\n",
              "      <td>0.933241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10000, training_loss=0.23283363342508673, metrics={'train_runtime': 4635.8288, 'train_samples_per_second': 17.257, 'train_steps_per_second': 2.157, 'total_flos': 1.059739189248e+16, 'train_loss': 0.23283363342508673, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n"
      ],
      "metadata": {
        "id": "INcUk1091rOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "9ab236de-a2ef-4385-e227-0189a5cd3cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 02:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.27816954255104065, 'eval_accuracy': 0.9322, 'eval_precision': 0.9217408274785324, 'eval_recall': 0.9446, 'eval_f1': 0.9330304227578032, 'eval_runtime': 147.4179, 'eval_samples_per_second': 67.834, 'eval_steps_per_second': 8.479, 'epoch': 2.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Chunking"
      ],
      "metadata": {
        "id": "Dyl2cGWkOd_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, tokenizer, max_length=512, overlap=50):\n",
        "\n",
        "    # Splits the text into multiple overlapping chunks of up to `max_length` tokens.\n",
        "    # Overlap ensures context continuity between chunks.\n",
        "    # Returns a list of text segments, each of which is <= max_length tokens.\n",
        "\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    chunked_texts = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(tokens):\n",
        "        chunk = tokens[start:start+max_length]\n",
        "        chunk_str = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "        chunked_texts.append(chunk_str)\n",
        "\n",
        "        start += (max_length - overlap)\n",
        "\n",
        "    return chunked_texts\n"
      ],
      "metadata": {
        "id": "Kin6z5iFOjSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_contents = []\n",
        "chunked_labels = []\n",
        "\n",
        "for content, label in zip(train_df['Cleaned_Content'], train_df['label']):\n",
        "    # Split the text into multiple 512-token chunks\n",
        "    segments = chunk_text(content, tokenizer, max_length=512, overlap=50)\n",
        "\n",
        "    # Each chunk is treated like a separate training example\n",
        "    for seg in segments:\n",
        "        chunked_contents.append(seg)\n",
        "        chunked_labels.append(label)\n",
        "\n",
        "val_chunked_contents = []\n",
        "val_chunked_labels = []\n",
        "\n",
        "for content, label in zip(val_df['Cleaned_Content'], val_df['label']):\n",
        "    segments = chunk_text(content, tokenizer, max_length=512, overlap=50)\n",
        "\n",
        "    for seg in segments:\n",
        "        val_chunked_contents.append(seg)\n",
        "        val_chunked_labels.append(label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNpIFlMTPsrC",
        "outputId": "641685d5-ee5b-4b28-e4ef-2a6b0d82f65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize training and validation data after chunking\n",
        "train_encodings = tokenizer(\n",
        "    chunked_contents,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    val_chunked_contents,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n"
      ],
      "metadata": {
        "id": "WYLMLYsIPxgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation datasets using the custom MovieReviewDataset class after chunking\n",
        "train_dataset = MovieReviewDataset(train_encodings, chunked_labels)\n",
        "val_dataset   = MovieReviewDataset(val_encodings, val_chunked_labels)\n"
      ],
      "metadata": {
        "id": "xu7jYqwZPz5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments, and saving the chunked model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/it1244/results_chunked',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "igcNjbM3P_yI",
        "outputId": "b903f435-7a7d-4f88-c698-30ef78de8561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11282' max='11282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11282/11282 1:14:03, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.213700</td>\n",
              "      <td>0.231006</td>\n",
              "      <td>0.917175</td>\n",
              "      <td>0.928303</td>\n",
              "      <td>0.906425</td>\n",
              "      <td>0.917233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>0.317412</td>\n",
              "      <td>0.922744</td>\n",
              "      <td>0.925193</td>\n",
              "      <td>0.921962</td>\n",
              "      <td>0.923575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11282, training_loss=0.24502035260545227, metrics={'train_runtime': 4444.3126, 'train_samples_per_second': 20.306, 'train_steps_per_second': 2.539, 'total_flos': 1.1954387924312064e+16, 'train_loss': 0.24502035260545227, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Freezing BERT and training two additional layers on top."
      ],
      "metadata": {
        "id": "-xf5CSdqXBlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom model that builds on a pre-trained DistilBERT encoder with a two-layer MLP head.\n",
        "class DistilBertWithTwoLayerHead(nn.Module):\n",
        "    # Custom model that:\n",
        "    # 1) Loads a pre-trained DistilBERT encoder and freezes its weights.\n",
        "    # 2) Adds a two-layer MLP classification head on top of the frozen encoder.\n",
        "    # 3) Computes the cross-entropy loss when labels are provided.\n",
        "\n",
        "    def __init__(self, dropout_rate=0.1, num_labels=2):\n",
        "        super(DistilBertWithTwoLayerHead, self).__init__()\n",
        "\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        for param in self.distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        hidden_size = self.distilbert.config.hidden_size\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_labels)\n",
        "        )\n",
        "\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs[0]\n",
        "        cls_rep = last_hidden_state[:, 0, :]\n",
        "\n",
        "        logits = self.classifier(cls_rep)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fct(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)\n"
      ],
      "metadata": {
        "id": "fLiyEa3XV7lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the custom model with a dropout rate of 0.1 and 2 output labels\n",
        "model = DistilBertWithTwoLayerHead(\n",
        "    dropout_rate=0.1,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/it1244/results_frozen_2layer',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs_frozen_2layer',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaE4h_HjVfNT",
        "outputId": "ae00dfed-362c-42c4-88f7-42235d73d31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "OCMYi4WIWBVf",
        "outputId": "b94d99a2-fedf-4eb9-b6ad-7587d539680e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 24:50, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.420800</td>\n",
              "      <td>0.347469</td>\n",
              "      <td>0.852100</td>\n",
              "      <td>0.886498</td>\n",
              "      <td>0.807600</td>\n",
              "      <td>0.845212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.391800</td>\n",
              "      <td>0.336869</td>\n",
              "      <td>0.856300</td>\n",
              "      <td>0.860555</td>\n",
              "      <td>0.850400</td>\n",
              "      <td>0.855447</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10000, training_loss=0.37996453809738157, metrics={'train_runtime': 1491.2845, 'train_samples_per_second': 53.645, 'train_steps_per_second': 6.706, 'total_flos': 0.0, 'train_loss': 0.37996453809738157, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}